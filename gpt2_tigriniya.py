# -*- coding: utf-8 -*-
"""GPT2_Tigriniya.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C5WdXRsHkVDQeZVen8plle4R59iWH6Wf

#Install & imports
"""

# Upgrade pip first
!pip install -q --upgrade pip

# Install core packages (latest stable)
!pip install -q "transformers" "peft" "accelerate" "safetensors" "datasets" "sentencepiece" "tokenizers"

#import os, sys
#os.kill(os.getpid(), 9)

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

import os, math, torch, numpy as np
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling,
    TrainingArguments, Trainer, pipeline
)
from peft import LoraConfig, get_peft_model

#!pip install -q transformers==4.41.2 datasets peft accelerate evaluate safetensors

import os, math, torch, numpy as np
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling,
    TrainingArguments, Trainer, pipeline
)
from peft import LoraConfig, get_peft_model



from transformers import AutoTokenizer

"""#Mount Drive & setting paths"""

# Colab: mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

#path & files
base_path = "/content/drive/MyDrive/Projects/LLM Projects"
train_path = os.path.join(base_path, "tig_train.txt")
val_path   = os.path.join(base_path, "tig_valid.txt")

print("Train path:", train_path)
print("Valid path:", val_path)
print("Train exists?", os.path.exists(train_path))
print("Valid exists?", os.path.exists(val_path))

# quick peek (handles UTF-8 Tigrinya)
if os.path.exists(train_path):
    with open(train_path, "r", encoding="utf-8", errors="ignore") as f:
        for _ in range(3):
            print("Sample:", f.readline().strip())

"""#Load the dataset or check dataset"""

data_files = {}
if os.path.exists(train_path): data_files["train"] = train_path
if os.path.exists(val_path):   data_files["validation"] = val_path

assert "train" in data_files, "Training file not found. Check path/filename."
ds = load_dataset("text", data_files=data_files)

ds

"""#Tokenizer ( GPT-2; pad token fix)"""

tokenizer = AutoTokenizer.from_pretrained("gpt2", use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({"pad_token": tokenizer.eos_token})
print("Vocab size:", len(tokenizer))

"""#Tokenize & chunk into blocks (causal LM)"""

block_size = 128  # shorter context to speed up training

def tok_fn(batch):
    return tokenizer(
        batch["text"],
        truncation=True,
        max_length=block_size,
        return_attention_mask=False
    )

# Tokenize dataset
tokenized = ds.map(tok_fn, batched=True, remove_columns=["text"])

def group_texts(examples):
    # Concatenate then split into block_size chunks
    concatenated = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated["input_ids"])
    total_length = (total_length // block_size) * block_size
    result = {
        k: [t[i:i+block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

lm_datasets = tokenized.map(group_texts, batched=True, batch_size=1000)

lm_datasets

# Reduce dataset size for faster training
small_train = lm_datasets["train"].shuffle(seed=42).select(range(10000))   # 10k examples
small_val   = lm_datasets["validation"].shuffle(seed=42).select(range(1000))  # 1k examples

print("Train size:", len(small_train))
print("Validation size:", len(small_val))

"""#Load base model & attach LoRA adapters"""

base_model_name = "gpt2"  # use "distilgpt2" for even faster runs
model = AutoModelForCausalLM.from_pretrained(base_model_name)
model.resize_token_embeddings(len(tokenizer))  # adjust embeddings if pad_token was added

lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["c_attn", "c_proj"],  # LoRA targets in GPT-2
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)
model = get_peft_model(model, lora_config)

trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
total = sum(p.numel() for p in model.parameters())
print(f"Trainable params: {trainable:,} / Total: {total:,} ({100*trainable/total:.2f}% trainable)")

"""#Training config"""

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

use_fp16 = torch.cuda.is_available()

training_args = TrainingArguments(
    output_dir="./ti-gpt2-lora",
    per_device_train_batch_size=2,      # keep small for T4 GPU
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,      # effective batch size = 8
    num_train_epochs=1,                 # just 1 epoch for demo (fast!)
    learning_rate=2e-4,                 # LoRA works well with slightly higher LR
    weight_decay=0.01,
    logging_steps=50,
    eval_strategy="epoch",        # run eval at end of each epoch
    save_strategy="no",                 # donâ€™t save multiple checkpoints (faster)
    fp16=use_fp16,                      # mixed precision if GPU supports
    bf16=False,
    report_to=[]                        # disable logging to W&B
)

"""#Train"""

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train,
    eval_dataset=small_val,
    data_collator=data_collator,
)

# Start training
trainer.train()

#trainer.save_model("./ti-gpt2-lora-final")  # saves LoRA adapter weights

"""#save model"""

save_dir = "/content/drive/MyDrive/Projects/LLM Projects/gpt2-tigrinya-lora"
trainer.save_model(save_dir)

save_dir

results = {}
if trainer.eval_dataset is not None:
    eval_out = trainer.evaluate()
    eval_loss = float(eval_out["eval_loss"])
    ppl = math.exp(eval_loss) if eval_loss < 100 else float("inf")
    results = {"eval_loss": eval_loss, "perplexity": ppl}
    print(results)
else:
    print("No validation dataset; skipped perplexity.")

"""#Evaluate (Perplexity)"""

results = {}
if "validation" in lm_datasets:
    eval_out = trainer.evaluate()
    eval_loss = float(eval_out["eval_loss"])
    ppl = math.exp(eval_loss) if eval_loss < 100 else float("inf")
    results = {"eval_loss": eval_loss, "perplexity": ppl}
    print(results)
else:
    print("No validation split found; skipped perplexity.")

"""#Generate Tigrinya text (sampling)"""

model.to("cuda" if torch.cuda.is_available() else "cpu")

# Load model & tokenizer
model_id = "/content/drive/MyDrive/Projects/LLM Projects/gpt2-tigrinya-lora"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1
)

# Chatbot logic (messages API)

def chatbot(user_message, history):
    """
    history is a list of {"role": ..., "content": ...}
    """
    if history is None:
        history = []

    # Build conversation text
    text = "System: You are a helpful assistant that responds in Tigrinya.\n"
    for msg in history:
        role = msg["role"]
        content = msg["content"]
        if role == "user":
            text += f"User: {content}\n"
        elif role == "assistant":
            text += f"Assistant: {content}\n"
    text += f"User: {user_message}\nAssistant:"

    # Generate response
    outs = generator(
        text,
        max_length=len(tokenizer(text)["input_ids"]) + 150,
        do_sample=True,
        temperature=0.7,
        top_k=50,
        top_p=0.9,
        num_return_sequences=1,
        pad_token_id=tokenizer.eos_token_id,
    )

    full_output = outs[0]["generated_text"]
    reply = full_output[len(text):].strip().split("\n")[0]

    # Return updated conversation
    history.append({"role": "user", "content": user_message})
    history.append({"role": "assistant", "content": reply})
    return history

# Gradio ChatInterface

chatbot_ui = gr.ChatInterface(
    fn=chatbot,
    type="messages",
    title="ðŸ’¬ Tigrinya GPT-2 LoRA Chatbot",
    description="A chatbot fine-tuned with LoRA on GPT-2. Responds in Tigrinya.",
    theme="soft",
)

if __name__ == "__main__":
    chatbot_ui.launch(share=True)

chatbot_ui.launch(share=True)
