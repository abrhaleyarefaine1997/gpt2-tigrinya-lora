{"cells":[{"cell_type":"markdown","metadata":{"id":"idl72iWAYKdh"},"source":["#Install \u0026 imports"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":34795,"status":"ok","timestamp":1756751227234,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"qR_T6wWq0-Y2"},"outputs":[],"source":["# Upgrade pip first\n","!pip install -q --upgrade pip\n","\n","# Install core packages (latest stable)\n","!pip install -q \"transformers\" \"peft\" \"accelerate\" \"safetensors\" \"datasets\" \"sentencepiece\" \"tokenizers\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NbFX9GaG2FHh"},"outputs":[],"source":["#import os, sys\n","#os.kill(os.getpid(), 9)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1756751227250,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"GKfDs0gR2QxG"},"outputs":[],"source":["import os, math, torch, numpy as np\n","from datasets import load_dataset\n","from transformers import (\n","    AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling,\n","    TrainingArguments, Trainer, pipeline\n",")\n","from peft import LoraConfig, get_peft_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_iFFZ5BBBN9"},"outputs":[],"source":["#!pip install -q transformers==4.41.2 datasets peft accelerate evaluate safetensors"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1756751227257,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"7fyUksMjVg27"},"outputs":[],"source":["import os, math, torch, numpy as np\n","from datasets import load_dataset\n","from transformers import (\n","    AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling,\n","    TrainingArguments, Trainer, pipeline\n",")\n","from peft import LoraConfig, get_peft_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_FAf1-BT2Dxk"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1756751227263,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"6oCkm5lBm7S2"},"outputs":[],"source":["from transformers import AutoTokenizer"]},{"cell_type":"markdown","metadata":{"id":"cJ-GXnNqXx3r"},"source":["#Mount Drive \u0026 setting paths"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1940,"status":"ok","timestamp":1756751229210,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"lVFhkMoVXGoR","outputId":"91ea594d-4826-4564-8903-7a8ad7b7b918"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Train path: /content/drive/MyDrive/Projects/LLM Projects/tig_train.txt\n","Valid path: /content/drive/MyDrive/Projects/LLM Projects/tig_valid.txt\n","Train exists? True\n","Valid exists? True\n","Sample: ኣብ ገለ እዋን መዓት ዝጸሓፍ ኣብ ኣእምሮይ ይመጽእ እሞ ፡ ክጽሕፍ ኢለ ብርዕን ወረቐትን ምስ ሓዝኩ ህልም ይብለኒ ።\n","Sample: ሽዑ እሓርቕ ።\n","Sample: ስለምንታይ እየ ክጽሕፍ ዘይክእል?\n"]}],"source":["# Colab: mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#path \u0026 files\n","base_path = \"/content/drive/MyDrive/Projects/LLM Projects\"\n","train_path = os.path.join(base_path, \"tig_train.txt\")\n","val_path   = os.path.join(base_path, \"tig_valid.txt\")\n","\n","print(\"Train path:\", train_path)\n","print(\"Valid path:\", val_path)\n","print(\"Train exists?\", os.path.exists(train_path))\n","print(\"Valid exists?\", os.path.exists(val_path))\n","\n","# quick peek (handles UTF-8 Tigrinya)\n","if os.path.exists(train_path):\n","    with open(train_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n","        for _ in range(3):\n","            print(\"Sample:\", f.readline().strip())\n"]},{"cell_type":"markdown","metadata":{"id":"MrgRScqcY5eQ"},"source":["#Load the dataset or check dataset"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":3965,"status":"ok","timestamp":1756751233193,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"SFAocLE6YE__"},"outputs":[],"source":["data_files = {}\n","if os.path.exists(train_path): data_files[\"train\"] = train_path\n","if os.path.exists(val_path):   data_files[\"validation\"] = val_path\n","\n","assert \"train\" in data_files, \"Training file not found. Check path/filename.\"\n","ds = load_dataset(\"text\", data_files=data_files)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1756751237522,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"eLvjMc8PROxq","outputId":"ca7c63f5-fec8-4917-bd15-3089fc87f606"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text'],\n","        num_rows: 1978213\n","    })\n","    validation: Dataset({\n","        features: ['text'],\n","        num_rows: 43335\n","    })\n","})"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["ds"]},{"cell_type":"markdown","metadata":{"id":"aljo72eYoBtZ"},"source":["#Tokenizer ( GPT-2; pad token fix)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1197,"status":"ok","timestamp":1756751242588,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"_9vbTLlSRO1a","outputId":"d4e68f33-20dd-4a2c-bbe5-1f353b38fa51"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocab size: 50257\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n","print(\"Vocab size:\", len(tokenizer))"]},{"cell_type":"markdown","metadata":{"id":"6An_vHebpK-w"},"source":["#Tokenize \u0026 chunk into blocks (causal LM)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":81},"executionInfo":{"elapsed":912697,"status":"ok","timestamp":1756752169735,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"XYEJEwA4msp7"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e75c26d013f49f38ba13e335d89d485","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1978213 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e2cbc2977189484295cd320c0a00bd0e","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/43335 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"edb9b39f8d644c05ab6836cece7a41e3","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1978213 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9dcab49d9794445eab95552874056f00","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/43335 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["block_size = 128  # shorter context to speed up training\n","\n","def tok_fn(batch):\n","    return tokenizer(\n","        batch[\"text\"],\n","        truncation=True,\n","        max_length=block_size,\n","        return_attention_mask=False\n","    )\n","\n","# Tokenize dataset\n","tokenized = ds.map(tok_fn, batched=True, remove_columns=[\"text\"])\n","\n","def group_texts(examples):\n","    # Concatenate then split into block_size chunks\n","    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n","    total_length = len(concatenated[\"input_ids\"])\n","    total_length = (total_length // block_size) * block_size\n","    result = {\n","        k: [t[i:i+block_size] for i in range(0, total_length, block_size)]\n","        for k, t in concatenated.items()\n","    }\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","    return result\n","\n","lm_datasets = tokenized.map(group_texts, batched=True, batch_size=1000)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1756752363431,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"QzVDMq9PmbP7"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'labels'],\n","        num_rows: 1422303\n","    })\n","    validation: Dataset({\n","        features: ['input_ids', 'labels'],\n","        num_rows: 29486\n","    })\n","})"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["lm_datasets"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":354,"status":"ok","timestamp":1756752374642,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"BaCEFrfEaBdb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train size: 10000\n","Validation size: 1000\n"]}],"source":["# Reduce dataset size for faster training\n","small_train = lm_datasets[\"train\"].shuffle(seed=42).select(range(10000))   # 10k examples\n","small_val   = lm_datasets[\"validation\"].shuffle(seed=42).select(range(1000))  # 1k examples\n","\n","print(\"Train size:\", len(small_train))\n","print(\"Validation size:\", len(small_val))"]},{"cell_type":"markdown","metadata":{"id":"U-y2G4DrpbD7"},"source":["#Load base model \u0026 attach LoRA adapters"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1055,"status":"ok","timestamp":1756752379361,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"4fKiqFbQaRnN"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trainable params: 811,008 / Total: 125,250,816 (0.65% trainable)\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n","  warnings.warn(\n"]}],"source":["base_model_name = \"gpt2\"  # use \"distilgpt2\" for even faster runs\n","model = AutoModelForCausalLM.from_pretrained(base_model_name)\n","model.resize_token_embeddings(len(tokenizer))  # adjust embeddings if pad_token was added\n","\n","lora_config = LoraConfig(\n","    r=8,\n","    lora_alpha=32,\n","    target_modules=[\"c_attn\", \"c_proj\"],  # LoRA targets in GPT-2\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","model = get_peft_model(model, lora_config)\n","\n","trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","total = sum(p.numel() for p in model.parameters())\n","print(f\"Trainable params: {trainable:,} / Total: {total:,} ({100*trainable/total:.2f}% trainable)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"onqx6-kqadpd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"I_cjVY5AadzW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0k3JeGI-ad3R"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"LZPOBCjqpyJn"},"source":["#Training config"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1756752727369,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"18NsgC5hjQ-f"},"outputs":[],"source":["data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n","use_fp16 = torch.cuda.is_available()\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./ti-gpt2-lora\",\n","    per_device_train_batch_size=2,      # keep small for T4 GPU\n","    per_device_eval_batch_size=2,\n","    gradient_accumulation_steps=4,      # effective batch size = 8\n","    num_train_epochs=1,                 # just 1 epoch for demo (fast!)\n","    learning_rate=2e-4,                 # LoRA works well with slightly higher LR\n","    weight_decay=0.01,\n","    logging_steps=50,\n","    eval_strategy=\"epoch\",        # run eval at end of each epoch\n","    save_strategy=\"no\",                 # don’t save multiple checkpoints (faster)\n","    fp16=use_fp16,                      # mixed precision if GPU supports\n","    bf16=False,\n","    report_to=[]                        # disable logging to W\u0026B\n",")"]},{"cell_type":"markdown","metadata":{"id":"ur45PmGTp9c_"},"source":["#Train"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"elapsed":13157399,"status":"ok","timestamp":1756766600129,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"SiyWxjVIqh7o"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n","  warnings.warn(warn_msg)\n","`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [1250/1250 5:28:07, Epoch 1/1]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eEpoch\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","      \u003cth\u003eValidation Loss\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e1.671900\u003c/td\u003e\n","      \u003ctd\u003e1.611328\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=1250, training_loss=1.745262841796875, metrics={'train_runtime': 19706.9485, 'train_samples_per_second': 0.507, 'train_steps_per_second': 0.063, 'total_flos': 659458621440000.0, 'train_loss': 1.745262841796875, 'epoch': 1.0})"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=small_train,\n","    eval_dataset=small_val,\n","    data_collator=data_collator,\n",")\n","\n","# Start training\n","trainer.train()"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":677,"status":"ok","timestamp":1756766601468,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"eu7PHQGPqp6S"},"outputs":[],"source":["trainer.save_model(\"./ti-gpt2-lora-final\")  # saves LoRA adapter weights"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":56},"executionInfo":{"elapsed":695118,"status":"ok","timestamp":1756767296589,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"0MMSM89_qs_K"},"outputs":[{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [500/500 13:04]\n","    \u003c/div\u003e\n","    "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.6113276481628418, 'perplexity': 5.0094576120228735}\n"]}],"source":["results = {}\n","if trainer.eval_dataset is not None:\n","    eval_out = trainer.evaluate()\n","    eval_loss = float(eval_out[\"eval_loss\"])\n","    ppl = math.exp(eval_loss) if eval_loss \u003c 100 else float(\"inf\")\n","    results = {\"eval_loss\": eval_loss, \"perplexity\": ppl}\n","    print(results)\n","else:\n","    print(\"No validation dataset; skipped perplexity.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NOXYMV00qtCG"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"VVuqGmlZqJho"},"source":["#Evaluate (Perplexity)"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":56},"executionInfo":{"elapsed":677109,"status":"ok","timestamp":1756767973703,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"8XnTTXiNmszr"},"outputs":[{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='1000' max='500' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [500/500 26:08]\n","    \u003c/div\u003e\n","    "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.6113276481628418, 'perplexity': 5.0094576120228735}\n"]}],"source":["results = {}\n","if \"validation\" in lm_datasets:\n","    eval_out = trainer.evaluate()\n","    eval_loss = float(eval_out[\"eval_loss\"])\n","    ppl = math.exp(eval_loss) if eval_loss \u003c 100 else float(\"inf\")\n","    results = {\"eval_loss\": eval_loss, \"perplexity\": ppl}\n","    print(results)\n","else:\n","    print(\"No validation split found; skipped perplexity.\")"]},{"cell_type":"markdown","metadata":{"id":"qBV88hPOqZ0Y"},"source":["#Generate Tigrinya text (sampling)"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1756767973706,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"in5X3Ncr7yqh"},"outputs":[{"data":{"text/plain":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): GPT2LMHeadModel(\n","      (transformer): GPT2Model(\n","        (wte): Embedding(50257, 768)\n","        (wpe): Embedding(1024, 768)\n","        (drop): Dropout(p=0.1, inplace=False)\n","        (h): ModuleList(\n","          (0-11): 12 x GPT2Block(\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (attn): GPT2Attention(\n","              (c_attn): lora.Linear(\n","                (base_layer): Conv1D(nf=2304, nx=768)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=768, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=2304, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (c_proj): lora.Linear(\n","                (base_layer): Conv1D(nf=768, nx=768)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=768, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=768, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (attn_dropout): Dropout(p=0.1, inplace=False)\n","              (resid_dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): GPT2MLP(\n","              (c_fc): Conv1D(nf=3072, nx=768)\n","              (c_proj): lora.Linear(\n","                (base_layer): Conv1D(nf=768, nx=3072)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=3072, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=768, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (act): NewGELUActivation()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n","    )\n","  )\n",")"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45855,"status":"ok","timestamp":1756768019563,"user":{"displayName":"Abrhaley Arefaine","userId":"10356780470326387619"},"user_tz":-120},"id":"EZgm1Umc8K00"},"outputs":[{"name":"stderr","output_type":"stream","text":["Device set to use cpu\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Both `max_new_tokens` (=256) and `max_length`(=160) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"name":"stdout","output_type":"stream","text":["\n","=== Sample 1 ===\n","ኣብ ኣዲስ ኣበባ ምስ ምሸት ኣብ መንገዲ ኣብ ገምጋም እየ። ንይንብን ሓገምት ሓስ ሓስ ምጋት። ካስትን፡ ኣብ ምሲኣብ ኣብ ዝሰንለዕ ከም ተቅብገነ ኣብ ነብዝት ንጋም ዝገምዕ ብምተም ነእባብ ዝሰንለዕ ካስትዝ ኣብ ዝሰንለዕ ዝደንለዕ ዝ�\n","\n","\n","=== Sample 2 ===\n","ኣብ ኣዲስ ኣበባ ምስ ምሸት ኣብ መንገዲ ኣብ ገምጋም እየ። ነነይት ውኣሰካት መንገዲ ኣብ ምጋምጋድል ኣብይት ውኣብትን ምስቲ ኣለንብትል ኣብይትት ኣብይትት ኣብምሳንስይስ ኣብምሰካትው ኣለንብስል ኣብይትል ኣብምሳን ኣምስል እ�\n","\n","\n","=== Sample 3 ===\n","ኣብ ኣዲስ ኣበባ ምስ ምሸት ኣብ መንገዲ ኣብ ገምጋም እየ። እይትም ደጸምሳታት እኒሰርነታት ድንብ ብትምው፡ ኣስበዲ እንገዲ ኣብንገዳታት ምሽስ እባተት እየ። ኣናኑሰ ኣእብእላነታትት እናናሰርነታት ኣምራዚ ካእየ። እባሃራ ነም�\n","\n"]}],"source":["generator = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    device=0 if torch.cuda.is_available() else -1\n",")\n","\n","prompt = \"ኣብ ኣዲስ ኣበባ ምስ ምሸት ኣብ መንገዲ ኣብ ገምጋም እየ።\"\n","outs = generator(\n","    prompt,\n","    max_length=160,\n","    do_sample=True,\n","    temperature=0.8,\n","    top_k=50,\n","    top_p=0.95,\n","    num_return_sequences=3\n",")\n","for i, o in enumerate(outs, 1):\n","    print(f\"\\n=== Sample {i} ===\\n{o['generated_text']}\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhkQSiBmms4N"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qeG3HWSqms6y"},"outputs":[{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2809772326.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m generator = pipeline(\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 8\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# your LoRA-wrapped GPT-2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["import gradio as gr\n","import torch\n","from transformers import pipeline\n","\n","# Load your model + tokenizer (already trained with LoRA)\n","generator = pipeline(\n","    \"text-generation\",\n","    model=model,          # your LoRA-wrapped GPT-2\n","    tokenizer=tokenizer,\n","    device=0 if torch.cuda.is_available() else -1\n",")\n","\n","# Helper: format conversation for GPT-2 style\n","def format_conversation(history):\n","    text = \"System: You are a helpful assistant that responds in Tigrinya.\\n\"\n","    for user_msg, bot_msg in history:\n","        text += f\"User: {user_msg}\\nAssistant: {bot_msg}\\n\"\n","    text += \"Assistant:\"\n","    return text\n","\n","# Generate reply\n","def chatbot(user_input, history):\n","    history = history or []  # initialize if empty\n","    prompt = format_conversation(history + [(user_input, \"\")])\n","\n","    outs = generator(\n","        prompt,\n","        max_length=200,\n","        do_sample=True,\n","        temperature=0.7,\n","        top_k=50,\n","        top_p=0.9,\n","        num_return_sequences=1,\n","        pad_token_id=generator.tokenizer.eos_token_id,\n","    )\n","\n","    full_output = outs[0][\"generated_text\"]\n","    reply = full_output[len(prompt):].strip().split(\"\\n\")[0]  # stop at first line\n","\n","    history.append((user_input, reply))\n","    return history, history\n","\n","# Gradio UI\n","chatbot_ui = gr.ChatInterface(\n","    fn=chatbot,\n","    title=\"💬 Tigrinya GPT-2 LoRA Chatbot\",\n","    description=\"A chatbot fine-tuned with LoRA on GPT-2. Responds in Tigrinya.\",\n","    theme=\"soft\",\n",")\n","\n","if __name__ == \"__main__\":\n","    chatbot_ui.launch()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"364srCeAms95"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM9UZH3KPUM1Pa+GZ8WaYet","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"07e4f0ae524848b6a64868cd7510a261":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c20bb14f0604b16ab1621e42092f365","placeholder":"​","style":"IPY_MODEL_e13ac5d08e814ca6b043b0e8a87a8e2c","value":" 1978213/1978213 [08:07\u0026lt;00:00, 4675.25 examples/s]"}},"28d3e34d6891460d8bd99c686bf8c01e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41635a7dade647cba13fbc8906d6d017":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b0a0db9a8fe4e9d9afe32ed8e253322":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bee8751cb0640759f7285fd7f2f3d7d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50a8d534591f4012bfb6137ebb9cad76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54767ad566a44d29b237175a9fb1dd8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b0659eed723d4caf95800825bdfafcff","IPY_MODEL_cc2fd9157b0745368db21129371eaccd","IPY_MODEL_a67928cd4693464688f4fa1e35168fdc"],"layout":"IPY_MODEL_d4c5571a271c44c3b3764e17f033ac14"}},"5682343e4e3742a5bb2c887044c63888":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_28d3e34d6891460d8bd99c686bf8c01e","max":1978213,"min":0,"orientation":"horizontal","style":"IPY_MODEL_67f5ad3fcb00459b9ba23417764e8086","value":1978213}},"5c20bb14f0604b16ab1621e42092f365":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67f5ad3fcb00459b9ba23417764e8086":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7ae5e45b287c430587fb2e0478cddbaf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a336dbcf275648a0a0ee226aca1b4ba5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4895e92c9c2432ca905c8611f217782":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a67928cd4693464688f4fa1e35168fdc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b0a0db9a8fe4e9d9afe32ed8e253322","placeholder":"​","style":"IPY_MODEL_50a8d534591f4012bfb6137ebb9cad76","value":" 1978213/1978213 [07:04\u0026lt;00:00, 4868.64 examples/s]"}},"b0659eed723d4caf95800825bdfafcff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4de96c3fcf043d0a4f699c13235115b","placeholder":"​","style":"IPY_MODEL_7ae5e45b287c430587fb2e0478cddbaf","value":"Map: 100%"}},"cc2fd9157b0745368db21129371eaccd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_41635a7dade647cba13fbc8906d6d017","max":1978213,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cf0f5a885da8405299d3410d2e19097e","value":1978213}},"cf0f5a885da8405299d3410d2e19097e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d4c5571a271c44c3b3764e17f033ac14":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4de96c3fcf043d0a4f699c13235115b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e13ac5d08e814ca6b043b0e8a87a8e2c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8c596c218934cac959e5cb24613fdc9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4895e92c9c2432ca905c8611f217782","placeholder":"​","style":"IPY_MODEL_4bee8751cb0640759f7285fd7f2f3d7d","value":"Map: 100%"}},"fbb67cc8d21f4c3c9a6df8645db787ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f8c596c218934cac959e5cb24613fdc9","IPY_MODEL_5682343e4e3742a5bb2c887044c63888","IPY_MODEL_07e4f0ae524848b6a64868cd7510a261"],"layout":"IPY_MODEL_a336dbcf275648a0a0ee226aca1b4ba5"}}}}},"nbformat":4,"nbformat_minor":0}